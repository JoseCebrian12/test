{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING EXP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# codificaciones = pd.read_excel(\n",
    "#     r\"cce/src/Data/Codificaciones.xlsx\", sheet_name=\"codificaciones\"\n",
    "# )\n",
    "codificaciones = pd.read_excel(\n",
    "    r\"/home/cnvdba/cce/src_dev/Data/Codificaciones.xlsx\", sheet_name=\"codificaciones\"\n",
    ")\n",
    "codificaciones.Code = codificaciones.Code.str.strip()\n",
    "rename_columns = {\n",
    "    \"creation_date\": \"creationDate\",\n",
    "    \"creation_time\": \"creationTime\",\n",
    "    \"creditor_participant_code\": \"creditorParticipantCode\",\n",
    "    \"debtor_participant_code\": \"debtorParticipantCode\",\n",
    "    \"debtor_type_person\": \"debtorTypeOfPerson\",\n",
    "    \"transaction_type\": \"transactionType\",\n",
    "    \"debtor_id\": \"debtorId\",\n",
    "    \"debtor_id_code\": \"debtorIdCode\",\n",
    "    \"creditor_cci\": \"creditorCCI\",\n",
    "    \"creditor_credit_card\": \"creditorCreditCard\",\n",
    "    \"reason_code\": \"reasonCode\",\n",
    "    \"response_code\": \"responseCode\",\n",
    "    \"creditor_id\": \"creditorId\",\n",
    "    \"creditor_id_code\": \"creditorIdCode\",\n",
    "}\n",
    "\n",
    "dict_column_list = {\n",
    "    \"debtorParticipantCode\": \"participants\",\n",
    "    \"creditorParticipantCode\": \"participants\",\n",
    "    \"transactionType\": \"transaction_type\",\n",
    "    \"currency\": \"currency\",\n",
    "    \"channel\": \"channel\",\n",
    "    \"responseCode\": \"response_code\",\n",
    "    \"reasonCode\": \"reason_code\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "_driver = \"PostgreSQL ANSI(x64)\"\n",
    "_server = \"localhost\" #\"10.201.4.25\" \n",
    "_database = \"coemcas\"\n",
    "_username = \"coemcas\"\n",
    "_password = \"C03$CMa5$2099\"\n",
    "_port = \"5432\"\n",
    "PREDICTION_PIPELINE = False\n",
    "NEW_VARIABLES_FLAG = True\n",
    "def get_database_conection():\n",
    "    connection = psycopg2.connect(\n",
    "        host=_server,\n",
    "        database=_database,\n",
    "        user=_username,\n",
    "        password=_password,\n",
    "        port=_port\n",
    "    )\n",
    "    return connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def data_ingestion_from_database():\n",
    "    #TODO: REVERT SQL QUERY\n",
    "    #Given the first unrun batch it finds, it retrieves all the transactions from the last 7 days (excluding the unrun batch) \n",
    "    if PREDICTION_PIPELINE:\n",
    "        #TODO: temporarily remove duplicate rows (by pk)\n",
    "        sql = \"\"\"\n",
    "            WITH run_id_to_process AS (\n",
    "                SELECT r.run_id, r.run_date, r.run_end_datetime\n",
    "                FROM fraud_model_run r\n",
    "                WHERE r.run_start_datetime = (\n",
    "                    SELECT MIN(r2.run_start_datetime)\n",
    "                    FROM fraud_model_run r2\n",
    "                    WHERE COALESCE(r2.run_process_status, 0) = 0\n",
    "                )\n",
    "            ),\n",
    "            debtor_ids AS (\n",
    "                SELECT DISTINCT m.debtor_id, r.run_id AS current_run_id, r.run_date AS current_run_date, r.run_end_datetime\n",
    "                FROM run_id_to_process r\n",
    "                INNER JOIN stage_ipf_message m ON r.run_id = m.run_id\n",
    "            )\n",
    "            SELECT DISTINCT ON (m.pk) m.*, d.current_run_id\n",
    "            FROM stage_ipf_message m \n",
    "            INNER JOIN debtor_ids d ON m.debtor_id = d.debtor_id\n",
    "            WHERE m.run_id = d.current_run_id\n",
    "            OR (\n",
    "                m.run_id != d.current_run_id\n",
    "                AND m.creation_date BETWEEN d.current_run_date - interval '90 days' AND d.current_run_date\n",
    "                AND (\n",
    "                    m.creation_date < d.current_run_date \n",
    "                    OR (m.creation_date = d.current_run_date AND m.creation_time <= d.run_end_datetime::time)\n",
    "                )\n",
    "            );\n",
    "        \"\"\"\n",
    "    else:\n",
    "        #TODO: delete \"limit 1000\" and change \"rn <= 20000\" to \"rn <= 200000\"\n",
    "        sql = \"\"\"\n",
    "            with debtor_ids as (\n",
    "                select debtor_id, current_creation_date\n",
    "                from (\n",
    "                    select \n",
    "                        debtor_id, \n",
    "                        current_creation_date,\n",
    "                        row_number() over (partition by current_creation_date order by random()) as rn\n",
    "                    from (\n",
    "                        select \n",
    "                            distinct m.debtor_id, \n",
    "                            m.creation_date as current_creation_date\n",
    "                        from stage_ipf_message m\n",
    "                        where m.creation_date between '2024-11-05' and '2024-11-11' limit 3000\n",
    "                    ) distinct_pairs\n",
    "                ) numbered_pairs\n",
    "                where rn <= 3000\n",
    "            ),\n",
    "            filtered_rows as (\n",
    "                select \n",
    "                    m.*, \n",
    "                    row_number() over (partition by m.pk) as row_num\n",
    "                from stage_ipf_message m\n",
    "                inner join debtor_ids d on m.debtor_id = d.debtor_id\n",
    "                where m.creation_date between d.current_creation_date - interval '90 days' and d.current_creation_date\n",
    "            )\n",
    "            select *\n",
    "            from filtered_rows\n",
    "            where row_num = 1;\n",
    "        \"\"\"\n",
    "\n",
    "    connection = None\n",
    "    # Establish the database connection\n",
    "    connection = get_database_conection()\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    print(f\"EXEC - Started query to get AV_consolidado rows at {datetime.datetime.now()}\")\n",
    "    cursor.execute(sql)\n",
    "    print(f\"EXEC - Ended query (cursor.execute) to get AV_consolidado rows at {datetime.datetime.now()}\")\n",
    "    # Obtener los nombres de las columnas\n",
    "    column_names = [desc[0] for desc in cursor.description]\n",
    "    # Obtener los resultados como una lista de listas\n",
    "    results = cursor.fetchall()\n",
    "    print(f\"EXEC - Ended query (cursor.fetchall) to get AV_consolidado rows at {datetime.datetime.now()}\")\n",
    "    cursor.close()\n",
    "    if connection is not None:\n",
    "        connection.close()\n",
    "    # Convertir los resultados en un DataFrame\n",
    "    AV_consolidado = pd.DataFrame(results, columns=column_names)\n",
    "\n",
    "    return AV_consolidado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXEC - Started query to get AV_consolidado rows at 2024-11-15 16:01:24.363784\n",
      "EXEC - Ended query (cursor.execute) to get AV_consolidado rows at 2024-11-15 16:01:25.480758\n",
      "EXEC - Ended query (cursor.fetchall) to get AV_consolidado rows at 2024-11-15 16:01:25.672762\n"
     ]
    }
   ],
   "source": [
    "AV_consolidado = data_ingestion_from_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if PREDICTION_PIPELINE:\n",
    "    max_run_id = AV_consolidado.iloc[0][\"current_run_id\"]\n",
    "else:\n",
    "    max_run_id = \"XXX\"\n",
    "drop_cols_av_cons = (\n",
    "    ['log_timestamp_replica', 'last_modified', 'current_run_id'] \n",
    "    if PREDICTION_PIPELINE \n",
    "    else ['log_timestamp_replica', 'last_modified', 'row_num']\n",
    ")\n",
    "AV_consolidado = AV_consolidado.drop(columns=drop_cols_av_cons)\n",
    "# AV_consolidado.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44897, 22)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AV_consolidado.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Crear lista negra DUMMY\n",
    "\n",
    "# Define a function to generate a list of unique strings\n",
    "def generate_unique_number_strings(count, length):\n",
    "    return [\"\".join(random.sample(\"0123456789\", length)) for _ in range(count)]\n",
    "\n",
    "# Initialize participant list and black_list dictionaries\n",
    "participant_list = [\n",
    "    \"0002\", \"0003\", \"0007\", \"0009\", \"0011\", \"0018\", \"0023\", \"0035\", \n",
    "    \"0038\", \"0043\", \"0049\", \"0053\", \"0054\", \"0055\", \"0058\", \"0094\", \n",
    "    \"0096\", \"0801\", \"0802\", \"0803\", \"0805\", \"0806\", \"0808\", \"0809\"\n",
    "]\n",
    "black_list = {\"origen\": {}, \"destino\": {}}\n",
    "\n",
    "# Populate black_list with random unique number strings\n",
    "for participant in participant_list:\n",
    "    for bl in black_list:\n",
    "        random_list = generate_unique_number_strings(10, 10)  # Generate 10 strings of 10 unique numbers\n",
    "        black_list[bl][participant] = random_list\n",
    "\n",
    "# Check results (printing the first few for brevity)\n",
    "# for k, v in list(black_list['origen'].items())[:5]:\n",
    "#     print(f\"{k}: {v}\")\n",
    "# print(\"_____________________\")\n",
    "# for k, v in list(black_list['destino'].items())[:5]:\n",
    "#     print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pk', 'debtor_id', 'creditor_cci', 'creation_date', 'creation_time',\n",
       "       'channel', 'currency', 'creditor_participant_code',\n",
       "       'debtor_participant_code', 'debtor_type_person', 'transaction_type',\n",
       "       'debtor_id_code', 'creditor_credit_card', 'reason_code',\n",
       "       'response_code', 'creditor_id', 'creditor_id_code', 'message_id',\n",
       "       'trace', 'instruction_id', 'run_id', 'same_customer_flag',\n",
       "       'personeria_debtor', 'personeria_creditor', 'transaction_amount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear personeria origen y destino DUMMY\n",
    "unique_debtors = AV_consolidado['debtor_id'].unique()\n",
    "unique_creditors = AV_consolidado['creditor_id'].unique()\n",
    "\n",
    "personeria_options = ['Natural', 'Juridica']\n",
    "new_columns = [\"personeria_debtor_natural\", \"personeria_creditor_natural\"]\n",
    "debtor_personeria_mapping = {debtor: np.random.choice(personeria_options) for debtor in unique_debtors}\n",
    "creditor_personeria_mapping = {creditor: np.random.choice(personeria_options) for creditor in unique_creditors}\n",
    "\n",
    "AV_consolidado['personeria_debtor'] = AV_consolidado['debtor_id'].map(debtor_personeria_mapping)\n",
    "AV_consolidado['personeria_creditor'] = AV_consolidado['creditor_id'].map(creditor_personeria_mapping)\n",
    "\n",
    "# Crear monto de transaccion DUMMY\n",
    "\n",
    "np.random.seed(0)  # Optional: for reproducibility\n",
    "AV_consolidado['transaction_amount'] = np.random.uniform(100, 1000, size=len(AV_consolidado))\n",
    "AV_consolidado.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Juridica\n",
       "1         Natural\n",
       "2         Natural\n",
       "3         Natural\n",
       "4         Natural\n",
       "           ...   \n",
       "44892     Natural\n",
       "44893     Natural\n",
       "44894     Natural\n",
       "44895    Juridica\n",
       "44896    Juridica\n",
       "Name: personeria_debtor, Length: 44897, dtype: object"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AV_consolidado.personeria_debtor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debtorParticipantCode\n",
      "creditorParticipantCode\n",
      "transactionType\n",
      "currency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3618309/4162895485.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  AV_consolidado[column_to_decode].fillna(\"\", inplace=True)\n",
      "/tmp/ipykernel_3618309/4162895485.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  AV_consolidado[column_to_decode].fillna(\"\", inplace=True)\n",
      "/tmp/ipykernel_3618309/4162895485.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  AV_consolidado[column_to_decode].fillna(\"\", inplace=True)\n",
      "/tmp/ipykernel_3618309/4162895485.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  AV_consolidado[column_to_decode].fillna(\"\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel\n",
      "responseCode\n",
      "reasonCode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3618309/4162895485.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  AV_consolidado[column_to_decode].fillna(\"\", inplace=True)\n",
      "/tmp/ipykernel_3618309/4162895485.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  AV_consolidado[column_to_decode].fillna(\"\", inplace=True)\n",
      "/tmp/ipykernel_3618309/4162895485.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  AV_consolidado[column_to_decode].fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#NUEVA\n",
    "# Renombrar columnas in-place\n",
    "AV_consolidado.rename(columns=rename_columns, inplace=True)\n",
    "# start = time.time()\n",
    "for column_to_decode, value in dict_column_list.items():\n",
    "    print(column_to_decode)\n",
    "    # Filtrar codificaciones relevantes\n",
    "    codificaciones_stage = codificaciones[codificaciones['List'] == value][[\"Code\", \"Value\"]]\n",
    "    codificaciones_dict = codificaciones_stage.set_index('Code')['Value'].to_dict()\n",
    "\n",
    "    # Rellenar valores nulos en la columna a decodificar\n",
    "    AV_consolidado[column_to_decode].fillna(\"\", inplace=True)\n",
    "\n",
    "    # Realizar el merge de forma más eficiente\n",
    "    AV_consolidado['temp'] = AV_consolidado[column_to_decode].map(codificaciones_dict)\n",
    "\n",
    "    if column_to_decode not in [\"reasonCode\"]:\n",
    "        AV_consolidado['temp'] = AV_consolidado['temp'].fillna(\"invalid\")\n",
    " \n",
    "    # Insertar la columna mapeada en el lugar adecuado\n",
    "    AV_consolidado[column_to_decode] = AV_consolidado['temp']\n",
    "    AV_consolidado.drop('temp', axis=1, inplace=True)\n",
    "# Renombrar columnas finales in-place\n",
    "AV_consolidado.rename(\n",
    "    columns={\n",
    "        \"debtorParticipantCode\": \"debtorParticipant\",\n",
    "        \"creditorParticipantCode\": \"creditorParticipant\",\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "# end = time.time()\n",
    "# print(\"TOT TIME\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2024-08-07\n",
       "1     2024-08-08\n",
       "2     2024-08-09\n",
       "3     2024-08-10\n",
       "4     2024-08-11\n",
       "         ...    \n",
       "92    2024-11-07\n",
       "93    2024-11-08\n",
       "94    2024-11-09\n",
       "95    2024-11-10\n",
       "96    2024-11-11\n",
       "Name: creationDate, Length: 97, dtype: object"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AV_consolidado.creationDate.drop_duplicates().sort_values().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "AV_consolidado[\"flag_invalid\"] = (AV_consolidado.astype(str).apply(lambda x: x == \"invalid\").any(axis=1)).astype(\"uint8\")\n",
    "AV_consolidado_original_values_set_stage = AV_consolidado.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "AV_consolidado.reset_index(drop=True, inplace=True)\n",
    "rows_dropped = (AV_consolidado_original_values_set_stage.shape[0] - AV_consolidado.shape[0])\n",
    "if rows_dropped > 0:\n",
    "    print(f\"{rows_dropped} rows dropped because any value was invalid\")\n",
    "AV_consolidado.drop([\"flag_invalid\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "vars_to_discard = [\n",
    "    \"pk\",\n",
    "    \"reasonCode\",\n",
    "    \"run_id\",\n",
    "    \"trace\",\n",
    "    \"instruction_id\",\n",
    "    \"message_id\",\n",
    "]  # 1\n",
    "\n",
    "vars_to_feature_engineer = [\"creationDate\", \"creationTime\"]  # 2\n",
    "vars_to_ohe = [\n",
    "    \"debtorTypeOfPerson\",\n",
    "    \"debtorParticipant\",\n",
    "    \"creditorParticipant\",\n",
    "    \"transactionType\",\n",
    "    \"currency\",\n",
    "    \"channel\",\n",
    "    \"responseCode\",\n",
    "    \"personeria_debtor\",\n",
    "    \"personeria_creditor\",\n",
    "]  # 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FRECUENCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#FRECUENCIA\n",
    "def create_frequency_features(df, new_cols, freq_days = [1, 7, 30, 90]):\n",
    "    df = df.sort_values(by=[\"debtorId\",\"creationDate\",\"creationTime\"]) #.dropna().copy()\n",
    "    # df[\"creation_date_temp2\"] = df[\"creation_date_temp\"]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.set_index('creation_date_temp2', inplace=True)\n",
    "    result_df = pd.DataFrame(index=df.index)\n",
    "    day_intervals = df[\"day_interval\"].unique()\n",
    "    for di in day_intervals:\n",
    "        df[f'di_is_{di}'] = (df['day_interval'] == di).astype(int)\n",
    "    for days in freq_days:\n",
    "        vals = [0] * len(df)\n",
    "        for di in day_intervals:\n",
    "            di_counts = (df.groupby('debtorId')[f'di_is_{di}']\n",
    "                        .rolling(window=f'{days}d')\n",
    "                        .sum()\n",
    "                        .reset_index(level=0, drop=True))\n",
    "\n",
    "            new_col = f\"f{di}_{days}d\"\n",
    "            res = di_counts / days\n",
    "            result_df[new_col] = res\n",
    "            vals = [x + y for x, y in zip(vals, res)]\n",
    "            new_cols.append(new_col)\n",
    "        new_col = f\"f{days}d\"\n",
    "        result_df[new_col] = vals\n",
    "        new_cols.append(new_col)\n",
    "        ################################### OG\n",
    "        # result = (df.groupby('debtorId')\n",
    "        #                 .rolling(window=f'{days}d', on='creation_date_temp2')\n",
    "        #                 .creation_date_temp2\n",
    "        #                 .count())  # Rolling count without resetting index\n",
    "        # result = result.reset_index(level=0, drop=True)\n",
    "        # new_col = f\"f{days}d\"\n",
    "        # result_df[new_col] = result.values / days\n",
    "        # new_cols.append(new_col)\n",
    "        ##################################\n",
    "    # print(result_df)\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "    for di in day_intervals:\n",
    "        df.drop(columns=f'di_is_{di}', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, result_df], axis=1)\n",
    "    return df, new_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CANTIDAD DE OPERACIONES DEL CLIENTE POR DIA Y POR CANAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# CANTIDAD DE OPERACIONES DEL CLIENTE POR DIA Y POR CANAL\n",
    "def rolling_total_count(group, days):\n",
    "    return group.rolling(window=f'{days}D').count()\n",
    "\n",
    "def create_frequency_per_channel(df, new_cols, freq_days=[1, 7, 30, 90]):\n",
    "    df = df.sort_values(by=[\"debtorId\", \"creationDate\", \"creationTime\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df.set_index('creation_date_temp', inplace=True)\n",
    "    df[\"clean_channel\"] = df[\"channel\"].astype(str).apply(unidecode).str.replace(\" \", \"_\", regex=\"False\").str.lower()\n",
    "    channel_types = df[\"clean_channel\"].unique()\n",
    "    for channel in channel_types:\n",
    "        df[f'channel_is_{channel}'] = (df['clean_channel'] == channel).astype(int)\n",
    "    for days in freq_days:\n",
    "        # Contar el total de eventos en la ventana de días\n",
    "        total_counts = (\n",
    "            df.groupby('debtorId')['clean_channel']\n",
    "            .apply(lambda group: rolling_total_count(group, days))\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        for channel in channel_types:\n",
    "            # Contar eventos por canal en la ventana de días\n",
    "            channel_counts = (\n",
    "                df.groupby('debtorId')[f'channel_is_{channel}']\n",
    "                .rolling(window=f'{days}D')\n",
    "                .sum()\n",
    "                .reset_index(level=0, drop=True)\n",
    "            )\n",
    "\n",
    "            # Calcular la proporción para ese canal y ventana de tiempo\n",
    "            new_col = f'prop_{channel}_{days}d'\n",
    "            df[new_col] = channel_counts / total_counts\n",
    "            new_cols.append(new_col)\n",
    "    for channel in channel_types:\n",
    "        df.drop(columns=f'channel_is_{channel}', inplace=True)\n",
    "    df.drop(columns=[\"clean_channel\"], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df, new_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNI DESTINO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# DNI DESTINO \n",
    "def create_frequency_interaction_creditor_id(df, new_cols, freq_days = [1, 7, 30, 90]):\n",
    "    df[\"creditorId\"] = df[\"creditorId\"].fillna(\"00000000\")\n",
    "    df = df.sort_values(by=[\"debtorId\",\"creditorId\", \"creationDate\",\"creationTime\"]) #.dropna().copy()\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    result_df = pd.DataFrame(index=df.index)\n",
    "    for days in freq_days:\n",
    "        # result = (df.groupby(['debtorId', 'creditorId'])\n",
    "        #                     .rolling(window=f'90d', on='creation_date_temp')\n",
    "        #                     .creation_date_temp\n",
    "        #                     .count().reset_index(drop=True))\n",
    "        result = (df.groupby(['debtorId', 'creditorId'])\n",
    "                .rolling(window=f'{days}d', on='creation_date_temp')\n",
    "                .creation_date_temp\n",
    "                .count())\n",
    "        result = result.reset_index(level=[0], drop=True)\n",
    "        new_col = f\"f{days}d_to_creditor\"\n",
    "        result_df[new_col] = result.values / days\n",
    "        new_cols.append(new_col)\n",
    "    # print(result_df)\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "    df = pd.concat([df, result_df], axis=1)\n",
    "    return df, new_cols   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUENTAS CON ABONOS DE DIFERENTES ORIGENES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#CUENTAS CON ABONOS DE DIFERENTES ORIGENES\n",
    "def create_unique_debtors_per_creditor(df, new_cols, freq_days = [1, 7, 30, 90]):    \n",
    "    # Sort by CreditorCCI, debtorId, and transaction date to ensure chronological order\n",
    "    df[\"creditorId\"] = df[\"creditorId\"].fillna(\"00000000\")\n",
    "    df = df.sort_values(by=[\"creditorId\", \"creationDate\",\"creationTime\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Create an empty DataFrame to store the results\n",
    "    result_df = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Use a rolling window per creditor to get unique debtorIds\n",
    "    # result = (df.groupby('creditorId')\n",
    "    #             .rolling(window=f'{window_days}d', on='creation_date_temp')\n",
    "    #             .apply(lambda x: x['debtorId'].unique(), raw=False)\n",
    "    #             )\n",
    "    df[\"hash_id\"] = pd.factorize(df[\"debtorId\"])[0]\n",
    "    # Crear ambos diccionarios en un solo bucle\n",
    "    # hash_in, hash_out = {}, {}\n",
    "    # for _, row in df.iterrows():\n",
    "    #     debtor_id = row[\"debtorId\"]\n",
    "    #     hash_id = row[\"hash_id\"]\n",
    "    #     hash_in[debtor_id] = hash_id\n",
    "    #     hash_out[hash_id] = debtor_id\n",
    "    for days in freq_days:\n",
    "        print(days)\n",
    "        # result = df.groupby('creditorId').apply(lambda x: x.rolling(window=f'{days}d', on='creation_date_temp').hash_id.apply(lambda y: y.nunique() ))\n",
    "        result = (df.groupby('creditorId').rolling(window=f'{days}d', on='creation_date_temp')['hash_id'].apply(lambda y: y.nunique()))\n",
    "        # Reset the index to avoid issues with multi-indexing\n",
    "        # result = result.reset_index(level=[0], drop=True)\n",
    "        result = result.reset_index(level=[0], drop=True).values\n",
    "\n",
    "        # Add the result to the original DataFrame\n",
    "        new_col = f\"unique_debtors_past_{days}d\"\n",
    "        result_df[new_col] = result\n",
    "        new_cols.append(new_col)\n",
    "        # Concatenate with the original DataFrame\n",
    "    df = pd.concat([df, result_df], axis=1)\n",
    "    df.drop(columns=[\"hash_id\"], inplace=True)\n",
    "    \n",
    "    return df, new_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROPORCION DE MONTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def amount_proportion(df, new_cols, freq_days = [1, 7, 30, 90]):\n",
    "    df = df.sort_values(by=[\"debtorId\", \"creationDate\",\"creationTime\"]) #.dropna().copy()\n",
    "    df = df.reset_index(drop=True)\n",
    "    result_df = pd.DataFrame(index=df.index)\n",
    "    for days in freq_days:\n",
    "        result = (df.groupby(['debtorId'])\n",
    "                .rolling(window=f'{days}d', on='creation_date_temp')\n",
    "                .transaction_amount\n",
    "                .sum())\n",
    "        result = result.reset_index(level=[0], drop=True)\n",
    "        new_col = f\"prop{days}d_amount\"\n",
    "        result_df[new_col] = df.transaction_amount / result.values\n",
    "        new_cols.append(new_col)\n",
    "    # print(result_df)\n",
    "    result_df = result_df.reset_index(drop=True)\n",
    "    df = pd.concat([df, result_df], axis=1)\n",
    "    return df, new_cols   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def categorize_day_interval(hour):\n",
    "    \"\"\"\n",
    "    Categorize the given hour into specific day intervals.\n",
    "\n",
    "    Args:\n",
    "        hour (int or str): The hour to be categorized. It can be an integer or a string representation of an integer.\n",
    "\n",
    "    Returns:\n",
    "        str: The category corresponding to the input hour.\n",
    "\n",
    "    \"\"\"\n",
    "    # Zero-padding if needed\n",
    "    hour = str(hour).zfill(6)  \n",
    "    # Extract the hour part\n",
    "    hour = int(hour[:2])  \n",
    "    # Define day interval categories\n",
    "    if hour >= 0 and hour < 6:\n",
    "        return 'early morning'\n",
    "    elif hour >= 6 and hour < 12:\n",
    "        return 'morning'\n",
    "    elif hour >= 12 and hour < 18:\n",
    "        return 'afternoon'\n",
    "    else:  # hour >= 18 or hour < 24\n",
    "        return 'evening'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "AV_consolidado[\"day_interval\"] = AV_consolidado[\"creationTime\"].apply(categorize_day_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "AV_consolidado[\"creation_date_temp\"] = pd.to_datetime(AV_consolidado[\"creationDate\"])\n",
    "temp = AV_consolidado.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first\n",
      "second\n",
      "1\n",
      "7\n",
      "30\n",
      "90\n",
      "third\n",
      "fourth\n",
      "fifth\n"
     ]
    }
   ],
   "source": [
    "#NEW VARIABLES\n",
    "if NEW_VARIABLES_FLAG:\n",
    "    AV_consolidado[\"creation_date_temp2\"] = pd.to_datetime(AV_consolidado[\"creationDate\"])\n",
    "    AV_consolidado[\"creation_date_temp\"] = pd.to_datetime(AV_consolidado[\"creationDate\"])\n",
    "    #in this exact order, to only create \"creation_date_temp\" once\n",
    "    AV_consolidado, new_columns = create_frequency_features(AV_consolidado, new_columns)\n",
    "    print(\"first\")\n",
    "    AV_consolidado, new_columns = create_frequency_interaction_creditor_id(AV_consolidado, new_columns)\n",
    "    print(\"second\")\n",
    "    AV_consolidado, new_columns = create_unique_debtors_per_creditor(AV_consolidado, new_columns)\n",
    "    print(\"third\")\n",
    "    AV_consolidado, new_columns = amount_proportion(AV_consolidado, new_columns)\n",
    "    print(\"fourth\")\n",
    "    AV_consolidado, new_columns = create_frequency_per_channel(AV_consolidado, new_columns)\n",
    "    print(\"fifth\")\n",
    "    AV_consolidado_original_values_set_stage = AV_consolidado.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# NEW\n",
    "# 1 vars_to_discard\n",
    "AV_consolidado = AV_consolidado.drop(columns=vars_to_discard)\n",
    "\n",
    "# 2 vars_to_feature_engineer\n",
    "AV_consolidado = AV_consolidado.rename(\n",
    "    columns={\n",
    "        \"creationDate\": \"creationDate_stage\",\n",
    "        \"creationTime\": \"creationTime_stage\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Conversión de fechas y horas\n",
    "AV_consolidado[\"creationDate_stage\"] = pd.to_datetime(AV_consolidado[\"creationDate_stage\"])\n",
    "AV_consolidado[\"creationTime_stage\"] = AV_consolidado[\"creationTime_stage\"].astype(str).str.replace(\":\", \"\", regex=False)\n",
    "AV_consolidado[\"creationTime\"] = pd.to_datetime(AV_consolidado[\"creationTime_stage\"], format=\"%H%M%S\").dt.time\n",
    "\n",
    "# Creación de características cíclicas\n",
    "def create_cyclic_features(df):\n",
    "    df[\"hourSin\"] = np.sin(2 * np.pi * df[\"creationTime\"].apply(lambda x: x.hour) / 24.0)\n",
    "    df[\"hourCos\"] = np.cos(2 * np.pi * df[\"creationTime\"].apply(lambda x: x.hour) / 24.0)\n",
    "    df[\"dayOfYearSin\"] = np.sin(2 * np.pi * df[\"creationDate_stage\"].dt.dayofyear / 365.0)\n",
    "    df[\"dayOfYearCos\"] = np.cos(2 * np.pi * df[\"creationDate_stage\"].dt.dayofyear / 365.0)\n",
    "    df[\"dayOfMonthSin\"] = np.sin(2 * np.pi * df[\"creationDate_stage\"].dt.day / 31.0)\n",
    "    df[\"dayOfMonthCos\"] = np.cos(2 * np.pi * df[\"creationDate_stage\"].dt.day / 31.0)\n",
    "    df[\"dayOfWeekSin\"] = np.sin(2 * np.pi * df[\"creationDate_stage\"].dt.weekday / 7.0)\n",
    "    df[\"dayOfWeekCos\"] = np.cos(2 * np.pi * df[\"creationDate_stage\"].dt.weekday / 7.0)\n",
    "    df[\"monthSin\"] = np.sin(2 * np.pi * df[\"creationDate_stage\"].dt.month / 12.0)\n",
    "    df[\"monthCos\"] = np.cos(2 * np.pi * df[\"creationDate_stage\"].dt.month / 12.0)\n",
    "    return df\n",
    "\n",
    "\n",
    "vars_to_feature_engineer = [\"creationTime\"]  # 2\n",
    "\n",
    "AV_consolidado = create_cyclic_features(AV_consolidado)\n",
    "\n",
    "# Eliminación de columnas intermedias\n",
    "AV_consolidado = AV_consolidado.drop(columns=[\"creationDate_stage\", \"creationTime_stage\", *vars_to_feature_engineer])\n",
    "\n",
    "# 3 vars_to_ohe\n",
    "AV_consolidado_3_stage = AV_consolidado.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Cambiar formato a \"snake_case\" sin tildes\n",
    "for column in vars_to_ohe:\n",
    "    AV_consolidado_3_stage[column] = (\n",
    "        AV_consolidado_3_stage[column]\n",
    "        .astype(str)\n",
    "        .apply(unidecode)\n",
    "        .str.replace(\" \", \"_\", regex=False)\n",
    "        .str.lower()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['debtorTypeOfPerson',\n",
       " 'debtorParticipant',\n",
       " 'creditorParticipant',\n",
       " 'transactionType',\n",
       " 'currency',\n",
       " 'channel',\n",
       " 'responseCode',\n",
       " 'personeria_debtor',\n",
       " 'personeria_creditor']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars_to_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cnvdba/miniconda3/envs/mvp1_shap/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(drop=&#x27;first&#x27;, handle_unknown=&#x27;ignore&#x27;, sparse=False,\n",
       "              sparse_output=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(drop=&#x27;first&#x27;, handle_unknown=&#x27;ignore&#x27;, sparse=False,\n",
       "              sparse_output=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(drop='first', handle_unknown='ignore', sparse=False,\n",
       "              sparse_output=False)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if PREDICTION_PIPELINE:\n",
    "    ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "else:\n",
    "    ohe = OneHotEncoder(sparse=False, handle_unknown=\"ignore\", drop=\"first\")\n",
    "ohe.fit(AV_consolidado_3_stage[vars_to_ohe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['none'], dtype=object),\n",
       " array(['alfin_banco_s.a.', 'banbif', 'banco_de_la_nacion',\n",
       "        'banco_falabella', 'banco_pichincha', 'banco_ripley', 'bbva',\n",
       "        'bcp', 'caja_arequipa', 'caja_cusco', 'caja_huancayo', 'caja_ica',\n",
       "        'caja_piura', 'caja_trujillo', 'crediscotia_financiera',\n",
       "        'financiera_oh', 'gnb', 'interbank', 'invalid', 'mi_banco',\n",
       "        'scotiabank'], dtype=object),\n",
       " array(['alfin_banco_s.a.', 'banbif', 'banco_de_la_nacion',\n",
       "        'banco_falabella', 'banco_pichincha', 'banco_ripley', 'bbva',\n",
       "        'bcp', 'caja_arequipa', 'caja_cusco', 'caja_huancayo', 'caja_ica',\n",
       "        'caja_piura', 'caja_trujillo', 'citibank', 'comercio',\n",
       "        'crediscotia_financiera', 'financiera_efectiva', 'financiera_oh',\n",
       "        'gnb', 'interbank', 'invalid', 'mi_banco', 'scotiabank'],\n",
       "       dtype=object),\n",
       " array(['ordinary_transfer', 'payments_to_account_card'], dtype=object),\n",
       " array(['dollars', 'soles'], dtype=object),\n",
       " array(['atm', 'banca_movil', 'invalid', 'web'], dtype=object),\n",
       " array(['accepted', 'rejected'], dtype=object),\n",
       " array(['juridica', 'natural'], dtype=object),\n",
       " array(['juridica', 'natural'], dtype=object)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "AV_consolidado_3_ohe_stage = pd.DataFrame(ohe.transform(AV_consolidado_3_stage[vars_to_ohe]),columns=ohe.get_feature_names_out(vars_to_ohe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "AV_consolidado = pd.concat([AV_consolidado_3_stage.drop(vars_to_ohe, axis=1), AV_consolidado_3_ohe_stage],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>debtorId</th>\n",
       "      <th>creditorCCI</th>\n",
       "      <th>channel</th>\n",
       "      <th>currency</th>\n",
       "      <th>creditorParticipant</th>\n",
       "      <th>debtorParticipant</th>\n",
       "      <th>debtorTypeOfPerson</th>\n",
       "      <th>transactionType</th>\n",
       "      <th>debtorIdCode</th>\n",
       "      <th>creditorCreditCard</th>\n",
       "      <th>...</th>\n",
       "      <th>hourSin</th>\n",
       "      <th>hourCos</th>\n",
       "      <th>dayOfYearSin</th>\n",
       "      <th>dayOfYearCos</th>\n",
       "      <th>dayOfMonthSin</th>\n",
       "      <th>dayOfMonthCos</th>\n",
       "      <th>dayOfWeekSin</th>\n",
       "      <th>dayOfWeekCos</th>\n",
       "      <th>monthSin</th>\n",
       "      <th>monthCos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000018298</td>\n",
       "      <td>00253519440189506733</td>\n",
       "      <td>invalid</td>\n",
       "      <td>soles</td>\n",
       "      <td>bcp</td>\n",
       "      <td>banco_de_la_nacion</td>\n",
       "      <td>none</td>\n",
       "      <td>ordinary_transfer</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-0.668064</td>\n",
       "      <td>-0.744104</td>\n",
       "      <td>0.651372</td>\n",
       "      <td>-0.758758</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000018298</td>\n",
       "      <td>00253519137531001137</td>\n",
       "      <td>invalid</td>\n",
       "      <td>soles</td>\n",
       "      <td>bcp</td>\n",
       "      <td>banco_de_la_nacion</td>\n",
       "      <td>none</td>\n",
       "      <td>ordinary_transfer</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>-0.785650</td>\n",
       "      <td>-0.618671</td>\n",
       "      <td>-0.968077</td>\n",
       "      <td>-0.250653</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000018298</td>\n",
       "      <td>00253519410805205636</td>\n",
       "      <td>invalid</td>\n",
       "      <td>soles</td>\n",
       "      <td>bcp</td>\n",
       "      <td>banco_de_la_nacion</td>\n",
       "      <td>none</td>\n",
       "      <td>ordinary_transfer</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>-9.659258e-01</td>\n",
       "      <td>-0.796183</td>\n",
       "      <td>-0.605056</td>\n",
       "      <td>-0.998717</td>\n",
       "      <td>-0.050649</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000018298</td>\n",
       "      <td>00253519218341307731</td>\n",
       "      <td>invalid</td>\n",
       "      <td>soles</td>\n",
       "      <td>bcp</td>\n",
       "      <td>banco_de_la_nacion</td>\n",
       "      <td>none</td>\n",
       "      <td>ordinary_transfer</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.816538</td>\n",
       "      <td>-0.577292</td>\n",
       "      <td>-0.937752</td>\n",
       "      <td>0.347305</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000018298</td>\n",
       "      <td>00253519699567404535</td>\n",
       "      <td>invalid</td>\n",
       "      <td>soles</td>\n",
       "      <td>bcp</td>\n",
       "      <td>banco_de_la_nacion</td>\n",
       "      <td>none</td>\n",
       "      <td>ordinary_transfer</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>9.659258e-01</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>-0.826354</td>\n",
       "      <td>-0.563151</td>\n",
       "      <td>-0.848644</td>\n",
       "      <td>0.528964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       debtorId           creditorCCI  channel currency creditorParticipant  \\\n",
       "0  000000018298  00253519440189506733  invalid    soles                 bcp   \n",
       "1  000000018298  00253519137531001137  invalid    soles                 bcp   \n",
       "2  000000018298  00253519410805205636  invalid    soles                 bcp   \n",
       "3  000000018298  00253519218341307731  invalid    soles                 bcp   \n",
       "4  000000018298  00253519699567404535  invalid    soles                 bcp   \n",
       "\n",
       "    debtorParticipant debtorTypeOfPerson    transactionType debtorIdCode  \\\n",
       "0  banco_de_la_nacion               none  ordinary_transfer            4   \n",
       "1  banco_de_la_nacion               none  ordinary_transfer            4   \n",
       "2  banco_de_la_nacion               none  ordinary_transfer            4   \n",
       "3  banco_de_la_nacion               none  ordinary_transfer            4   \n",
       "4  banco_de_la_nacion               none  ordinary_transfer            4   \n",
       "\n",
       "  creditorCreditCard  ...       hourSin       hourCos dayOfYearSin  \\\n",
       "0               None  ... -1.000000e+00 -1.836970e-16    -0.668064   \n",
       "1               None  ... -7.071068e-01 -7.071068e-01    -0.785650   \n",
       "2               None  ... -2.588190e-01 -9.659258e-01    -0.796183   \n",
       "3               None  ...  1.224647e-16 -1.000000e+00    -0.816538   \n",
       "4               None  ...  9.659258e-01 -2.588190e-01    -0.826354   \n",
       "\n",
       "  dayOfYearCos dayOfMonthSin dayOfMonthCos  dayOfWeekSin dayOfWeekCos  \\\n",
       "0    -0.744104      0.651372     -0.758758      0.000000     1.000000   \n",
       "1    -0.618671     -0.968077     -0.250653      0.433884    -0.900969   \n",
       "2    -0.605056     -0.998717     -0.050649     -0.433884    -0.900969   \n",
       "3    -0.577292     -0.937752      0.347305     -0.781831     0.623490   \n",
       "4    -0.563151     -0.848644      0.528964      0.000000     1.000000   \n",
       "\n",
       "   monthSin  monthCos  \n",
       "0 -0.866025      -0.5  \n",
       "1 -0.866025      -0.5  \n",
       "2 -0.866025      -0.5  \n",
       "3 -0.866025      -0.5  \n",
       "4 -0.866025      -0.5  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AV_consolidado_3_stage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "AV_consolidado[AV_consolidado_3_ohe_stage.columns] = AV_consolidado[AV_consolidado_3_ohe_stage.columns].astype(\"uint8\")\n",
    "AV_consolidado_completed = pd.DataFrame()\n",
    "AV_consolidado_completed = pd.concat([AV_consolidado_completed, AV_consolidado], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "AV_consolidado_original = pd.DataFrame()\n",
    "AV_consolidado_original_stage = AV_consolidado.copy()\n",
    "AV_consolidado_original = pd.concat([AV_consolidado_original, AV_consolidado_original_stage],axis=0,ignore_index=True,)\n",
    "AV_consolidado_original_values_set = pd.DataFrame()\n",
    "AV_consolidado_original_values_set = pd.concat([AV_consolidado_original_values_set, AV_consolidado_original_values_set_stage],axis=0,ignore_index=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Fill nulls\n",
    "AV_consolidado_completed.fillna(value=0, inplace=True)\n",
    "\n",
    "# Set all binary to uint8 (except the ones that are uint8 already)\n",
    "id_columns = [\n",
    "    \"debtorId\",\n",
    "    \"debtorIdCode\",\n",
    "    \"creditorCCI\",\n",
    "    \"creditorCreditCard\",\n",
    "    \"creditorId\",\n",
    "    \"creditorIdCode\",\n",
    "]\n",
    "\n",
    "excluded_columns = (\n",
    "    [\n",
    "        \"hourSin\",\n",
    "        \"hourCos\",\n",
    "        \"dayOfYearSin\",\n",
    "        \"dayOfYearCos\",\n",
    "        \"dayOfMonthSin\",\n",
    "        \"dayOfMonthCos\",\n",
    "        \"dayOfWeekSin\",\n",
    "        \"dayOfWeekCos\",\n",
    "        \"monthSin\",\n",
    "        \"monthCos\",\n",
    "        \"same_customer_flag\",\n",
    "        \"transaction_amount\",\n",
    "        \"day_interval\"\n",
    "    ]\n",
    "    + AV_consolidado_completed.select_dtypes(include=\"uint8\").columns.to_list()\n",
    "    + id_columns \n",
    ")\n",
    "if NEW_VARIABLES_FLAG:\n",
    "    excluded_columns +=  new_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['personeria_debtor_natural',\n",
       " 'personeria_creditor_natural',\n",
       " 'fevening_1d',\n",
       " 'fafternoon_1d',\n",
       " 'fmorning_1d',\n",
       " 'fearly morning_1d',\n",
       " 'f1d',\n",
       " 'fevening_7d',\n",
       " 'fafternoon_7d',\n",
       " 'fmorning_7d',\n",
       " 'fearly morning_7d',\n",
       " 'f7d',\n",
       " 'fevening_30d',\n",
       " 'fafternoon_30d',\n",
       " 'fmorning_30d',\n",
       " 'fearly morning_30d',\n",
       " 'f30d',\n",
       " 'fevening_90d',\n",
       " 'fafternoon_90d',\n",
       " 'fmorning_90d',\n",
       " 'fearly morning_90d',\n",
       " 'f90d',\n",
       " 'f1d_to_creditor',\n",
       " 'f7d_to_creditor',\n",
       " 'f30d_to_creditor',\n",
       " 'f90d_to_creditor',\n",
       " 'unique_debtors_past_1d',\n",
       " 'unique_debtors_past_7d',\n",
       " 'unique_debtors_past_30d',\n",
       " 'unique_debtors_past_90d',\n",
       " 'prop1d_amount',\n",
       " 'prop7d_amount',\n",
       " 'prop30d_amount',\n",
       " 'prop90d_amount',\n",
       " 'prop_invalid_1d',\n",
       " 'prop_banca_movil_1d',\n",
       " 'prop_web_1d',\n",
       " 'prop_atm_1d',\n",
       " 'prop_invalid_7d',\n",
       " 'prop_banca_movil_7d',\n",
       " 'prop_web_7d',\n",
       " 'prop_atm_7d',\n",
       " 'prop_invalid_30d',\n",
       " 'prop_banca_movil_30d',\n",
       " 'prop_web_30d',\n",
       " 'prop_atm_30d',\n",
       " 'prop_invalid_90d',\n",
       " 'prop_banca_movil_90d',\n",
       " 'prop_web_90d',\n",
       " 'prop_atm_90d']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BINARY []\n"
     ]
    }
   ],
   "source": [
    "# Get a list of column names excluding 'excluded_columns' and id columns\n",
    "binary_columns = list(set(AV_consolidado_completed.columns).difference(excluded_columns))\n",
    "print(\"BINARY\", binary_columns)\n",
    "\n",
    "# Convert the binary columns to uint8\n",
    "AV_consolidado_completed[binary_columns] = AV_consolidado_completed[binary_columns].astype(\"uint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# # Export AV_consolidado_completed\n",
    "# output_file_path = \"cce_ipf_message_feature_engineering.pickle\"\n",
    "# AV_consolidado_completed.to_pickle(output_file_path)\n",
    "\n",
    "# # Export AV_consolidado_original\n",
    "# output_file_path = \"cce_ipf_message_original.pickle\"\n",
    "# AV_consolidado_original.to_pickle(output_file_path)\n",
    "\n",
    "# # Export AV_consolidado_original_values_set\n",
    "# output_file_path = \"cce_ipf_message_original_values_set.pickle\"\n",
    "# AV_consolidado_original_values_set.to_pickle(output_file_path)\n",
    "# print(\"end of execution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44897, 74)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AV_consolidado_original_values_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44897, 118)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AV_consolidado_completed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING RATIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def generate_combinations(input_list):\n",
    "    \"\"\"\n",
    "    Generate all possible combinations of elements from the given input list.\n",
    "\n",
    "    The function generates combinations by first including each element in a separate list,\n",
    "    and then combining two elements at a time to form new combinations.\n",
    "\n",
    "    Args:\n",
    "        input_list (list): The list of elements from which to generate combinations.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing all possible combinations of elements.\n",
    "    \"\"\"\n",
    "    \n",
    "    output_list = []\n",
    "\n",
    "    for i in range(len(input_list)):\n",
    "        # Add each element in a list of 1 item\n",
    "        output_list.append([input_list[i]])\n",
    "\n",
    "        for j in range(i + 1, len(input_list)):\n",
    "            # Add combinations of two elements\n",
    "            output_list.append([input_list[i], input_list[j]])\n",
    "\n",
    "    return output_list\n",
    "\n",
    "def categorize_hour(hour):\n",
    "    \"\"\"\n",
    "    Categorize the given hour into specific time intervals.\n",
    "\n",
    "    Args:\n",
    "        hour (int or str): The hour to be categorized. It can be an integer or a string representation of an integer.\n",
    "\n",
    "    Returns:\n",
    "        str: The category corresponding to the input hour.\n",
    "\n",
    "    \"\"\"\n",
    "    # Zero-padding if needed\n",
    "    hour = str(hour).zfill(6)  \n",
    "    # Extract the hour part\n",
    "    hour = int(hour[:2])  \n",
    "    # Define time interval categories\n",
    "    if hour >= 0 and hour < 3:\n",
    "        return '00 to 03'\n",
    "    elif hour >= 3 and hour < 6:\n",
    "        return '03 to 06'\n",
    "    elif hour >= 6 and hour < 9:\n",
    "        return '06 to 09'\n",
    "    elif hour >= 9 and hour < 12:\n",
    "        return '09 to 12'\n",
    "    elif hour >= 12 and hour < 15:\n",
    "        return '12 to 15'\n",
    "    elif hour >= 15 and hour < 18:\n",
    "        return '15 to 18'\n",
    "    elif hour >= 18 and hour < 21:\n",
    "        return '18 to 21'\n",
    "    else:  # hour >= 21 or hour < 24\n",
    "        return '21 to 00'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Define a list of columns to keep in the resulting DataFrame\n",
    "lista = [\n",
    "    \"debtorId\",\n",
    "    \"creditorCCI\",\n",
    "    \"run_id\",\n",
    "    \"creditorParticipant\",\n",
    "    \"currency\",\n",
    "    \"channel\",\n",
    "    \"responseCode\",\n",
    "    \"debtorParticipant\",\n",
    "    \"creationDate\",\n",
    "    \"creationTime\",\n",
    "    \"time_interval\",\n",
    "    \"Weekday\",\n",
    "]\n",
    "\n",
    "# Generate all possible combinations of columns to be used for ratio calculations\n",
    "\n",
    "input_list = [\n",
    "    \"creditorParticipant\",\n",
    "    \"currency\",\n",
    "    \"channel\",\n",
    "    \"responseCode\",\n",
    "    \"debtorParticipant\",\n",
    "    \"Weekday\",\n",
    "    \"time_interval\",\n",
    "    \"creditorCCI\",\n",
    "]\n",
    "\n",
    "if NEW_VARIABLES_FLAG:\n",
    "    lista += [\"creditorId\",\"personeria_creditor\"]\n",
    "    input_list += [\"creditorId\", \"personeria_creditor\"]\n",
    "\n",
    "output_list = generate_combinations(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#antes = 45 \n",
    "# dsps\n",
    "len(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "directory_path = \"\"\n",
    "# Define the filenames for feature engineering and original values set DataFrames\n",
    "file_name_feature_engineering = \"cce_ipf_message_feature_engineering.pickle\"\n",
    "file_name_original_values_set = \"cce_ipf_message_original_values_set.pickle\"\n",
    "\n",
    "# Build the file paths for the feature engineering and original values set DataFrames\n",
    "file_path_fe = os.path.join(directory_path, file_name_feature_engineering)\n",
    "file_path_o_vs = os.path.join(directory_path, file_name_original_values_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# # Export AV_consolidado_completed\n",
    "# output_file_path = \"cce_ipf_message_feature_engineering.pickle\"\n",
    "# AV_consolidado_completed.to_pickle(output_file_path)\n",
    "\n",
    "# # Export AV_consolidado_original\n",
    "# output_file_path = \"cce_ipf_message_original.pickle\"\n",
    "# AV_consolidado_original.to_pickle(output_file_path)\n",
    "\n",
    "# # Export AV_consolidado_original_values_set\n",
    "# output_file_path = \"cce_ipf_message_original_values_set.pickle\"\n",
    "# AV_consolidado_original_values_set.to_pickle(output_file_path)\n",
    "# print(\"end of execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3618309/1182050474.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  AV_o_vs[\"creationDate\"] = pd.to_datetime(AV_o_vs[\"creationDate\"])\n"
     ]
    }
   ],
   "source": [
    "# Read the original values set DataFrame\n",
    "# AV_o_vs = pd.read_pickle(file_path_o_vs)\n",
    "AV_o_vs = AV_consolidado_original_values_set\n",
    "# Filter and preprocess the original values set DataFrame\n",
    "AV_o_vs = AV_o_vs[AV_o_vs[\"transactionType\"] == \"Ordinary Transfer\"]\n",
    "# AV_o_vs = AV_o_vs[AV_o_vs['debtorParticipant'].isin(code) | AV_o_vs['creditorParticipant'].isin(code)]\n",
    "AV_o_vs[\"creationDate\"] = pd.to_datetime(AV_o_vs[\"creationDate\"])\n",
    "df = AV_o_vs.copy()\n",
    "df[\"Weekday\"] = df[\"creationDate\"].apply(lambda x: x.weekday()).astype(object)\n",
    "df[\"time_interval\"] = df[\"creationTime\"].apply(categorize_hour)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pk', 'debtorId', 'creditorCCI', 'creationDate', 'creationTime',\n",
       "       'channel', 'currency', 'creditorParticipant', 'debtorParticipant',\n",
       "       'debtorTypeOfPerson', 'transactionType', 'debtorIdCode',\n",
       "       'creditorCreditCard', 'reasonCode', 'responseCode', 'creditorId',\n",
       "       'creditorIdCode', 'message_id', 'trace', 'instruction_id', 'run_id',\n",
       "       'same_customer_flag', 'personeria_debtor', 'personeria_creditor',\n",
       "       'transaction_amount', 'day_interval', 'fevening_1d', 'fafternoon_1d',\n",
       "       'fmorning_1d', 'fearly morning_1d', 'f1d', 'fevening_7d',\n",
       "       'fafternoon_7d', 'fmorning_7d', 'fearly morning_7d', 'f7d',\n",
       "       'fevening_30d', 'fafternoon_30d', 'fmorning_30d', 'fearly morning_30d',\n",
       "       'f30d', 'fevening_90d', 'fafternoon_90d', 'fmorning_90d',\n",
       "       'fearly morning_90d', 'f90d', 'f1d_to_creditor', 'f7d_to_creditor',\n",
       "       'f30d_to_creditor', 'f90d_to_creditor', 'unique_debtors_past_1d',\n",
       "       'unique_debtors_past_7d', 'unique_debtors_past_30d',\n",
       "       'unique_debtors_past_90d', 'prop1d_amount', 'prop7d_amount',\n",
       "       'prop30d_amount', 'prop90d_amount', 'prop_invalid_1d',\n",
       "       'prop_banca_movil_1d', 'prop_web_1d', 'prop_atm_1d', 'prop_invalid_7d',\n",
       "       'prop_banca_movil_7d', 'prop_web_7d', 'prop_atm_7d', 'prop_invalid_30d',\n",
       "       'prop_banca_movil_30d', 'prop_web_30d', 'prop_atm_30d',\n",
       "       'prop_invalid_90d', 'prop_banca_movil_90d', 'prop_web_90d',\n",
       "       'prop_atm_90d'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AV_consolidado_original_values_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44897"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AV_consolidado_original_values_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AV_consolidado_original_values_set[AV_consolidado_original_values_set[\"transactionType\"] != \"Ordinary Transfer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def Ratio(dd1, output_list, lista, days_back): #revisar la fecha\n",
    "    \"\"\"\n",
    "    Calculate ratios based on cumulative counts for specified columns in the input DataFrame.\n",
    " \n",
    "    The function takes a DataFrame dd1 and calculates ratios for combinations of columns specified in output_list.\n",
    "    The calculation is performed based on cumulative counts of each combination relative to a total count.\n",
    " \n",
    "    Args:\n",
    "        dd1 (pandas.DataFrame): The input DataFrame containing the data.\n",
    "        output_list (list): A list of lists, where each inner list represents a combination of columns for which ratios\n",
    "                            are to be calculated based on cumulative counts. The columns in each combination should be\n",
    "                            present in the DataFrame dd1.\n",
    "        lista (list): A list of column names from the DataFrame dd1 that will be used to extract data.\n",
    " \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the calculated ratios for each combination specified in output_list.\n",
    "    \"\"\"\n",
    "    creditor =[\"debtorId\"]\n",
    "    dd2_completed = pd.DataFrame()\n",
    " \n",
    "    total_days = dd1.creationDate.drop_duplicates().sort_values().reset_index(drop=True)\n",
    "    # print(total_days)\n",
    "    if PREDICTION_PIPELINE:\n",
    "        training_days = [total_days.max()]\n",
    "    else:\n",
    "        training_days = total_days[total_days>= '2024-11-05'].reset_index(drop=True)\n",
    "        \n",
    "    for i, _day in enumerate(training_days):\n",
    "        print(f\"processing day {i+1} of {len(training_days)}: {_day}\")\n",
    "        dd2=dd1[(dd1.creationDate <= _day) & (dd1.creationDate >= _day - pd.Timedelta(days=90))][lista]\\\n",
    "            .sort_values(by=[\"debtorId\",\"creationDate\",\"creationTime\"]).dropna().copy()\n",
    "        dd2[\"count_cci\"]=dd2.groupby([\"debtorId\"]).cumcount() + 1\n",
    "        for i in output_list:\n",
    "            dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
    "            dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
    "        #TODO: EL FILTRO DE RUN_ID YA NO SERIA, SERIA EL MAX CREATION DATE\n",
    "        print(\"SIZE DD2 (before)\", days_back, dd2.shape)\n",
    "        if PREDICTION_PIPELINE:\n",
    "            dd2 = dd2[dd2.run_id == max(dd2.run_id)]\n",
    "        else:\n",
    "            dd2 = dd2[dd2.creationDate == _day]\n",
    "        print(\"SIZE DD2 (after)\", days_back, dd2.shape)\n",
    "        dd2_completed = pd.concat([dd2_completed, dd2], axis=0, ignore_index=False)\n",
    "        dd2_completed = pd.concat([dd2_completed, dd2], axis=0, ignore_index=False)\n",
    "    return(dd2_completed.filter(like=\"ratio\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing day 1 of 7: 2024-11-05 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE DD2 (before) 90 (39531, 125)\n",
      "SIZE DD2 (after) 90 (953, 125)\n",
      "processing day 2 of 7: 2024-11-06 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE DD2 (before) 90 (40182, 125)\n",
      "SIZE DD2 (after) 90 (815, 125)\n",
      "processing day 3 of 7: 2024-11-07 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE DD2 (before) 90 (40823, 125)\n",
      "SIZE DD2 (after) 90 (842, 125)\n",
      "processing day 4 of 7: 2024-11-08 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE DD2 (before) 90 (41361, 125)\n",
      "SIZE DD2 (after) 90 (843, 125)\n",
      "processing day 5 of 7: 2024-11-09 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE DD2 (before) 90 (41720, 125)\n",
      "SIZE DD2 (after) 90 (618, 125)\n",
      "processing day 6 of 7: 2024-11-10 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE DD2 (before) 90 (42066, 125)\n",
      "SIZE DD2 (after) 90 (503, 125)\n",
      "processing day 7 of 7: 2024-11-11 00:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIZE DD2 (before) 90 (42497, 125)\n",
      "SIZE DD2 (after) 90 (743, 125)\n",
      "(10634, 55)\n",
      "(10634, 55)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n",
      "/tmp/ipykernel_3618309/2758591593.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_cumcount\"]=dd2.groupby(creditor+i).cumcount() + 1 #more precise\n",
      "/tmp/ipykernel_3618309/2758591593.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dd2['_'.join(map(str, i))+\"_ratio\"]=dd2['_'.join(map(str, i))+\"_cumcount\"]/dd2[\"count_cci\"]\n"
     ]
    }
   ],
   "source": [
    "# Calculate ratios based on cumulative counts for specified columns\n",
    "df[\"creditorId\"] = df[\"creditorId\"].fillna(\"00000000\")\n",
    "ratios_df = Ratio(df, output_list, lista, 90)\n",
    "print(ratios_df.shape)\n",
    "ratios_df.dropna(inplace=True)\n",
    "print(ratios_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Read the feature engineering DataFrame\n",
    "# AV_fe = pd.read_pickle(file_path_fe)\n",
    "AV_fe = AV_consolidado_completed\n",
    "AV_fe['in_black_debtor'] = AV_fe['debtorId'].apply(lambda x: any(x in bl for bl in black_list['origen'].values()))\n",
    "AV_fe['in_black_creditor'] = AV_fe['debtorId'].apply(lambda x: any(x in bl for bl in black_list['destino'].values()))\n",
    "AV_fe = AV_fe.drop([\"debtorIdCode\", \"creditorCreditCard\", \"creditorId\", \"creditorIdCode\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#TODO: later after training/predicting, like with same_customer_flag, add the rows back. but now, with score = 99.\n",
    "# if some rows have same_customer_flag = 'M' but either debtor or creditor are in any black list, that row has score = 99.\n",
    "\n",
    "black_df = AV_fe[AV_fe['in_black_debtor'] | AV_fe['in_black_creditor']]  # Rows where either in_origen or in_destino is True\n",
    "good_df = AV_fe[~(AV_fe['in_black_debtor'] | AV_fe['in_black_creditor'])]  # Rows where both in_origen and in_destino are False\n",
    "AV_fe = good_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10634, 171)\n",
      "(2326, 171)\n",
      "(8308, 171)\n",
      "(8308, 171)\n"
     ]
    }
   ],
   "source": [
    "# Merge the calculated ratios DataFrame and the feature engineering DataFrame\n",
    "final = pd.merge(ratios_df, AV_fe, left_index=True, right_index=True)\n",
    "\n",
    "final_same = final[final[\"same_customer_flag\"] == 'M']\n",
    "final_diff = final[final[\"same_customer_flag\"] != 'M']\n",
    "print(final.shape)\n",
    "print(final_same.shape)\n",
    "print(final_diff.shape)\n",
    "final = final_diff\n",
    "print(final.shape)\n",
    "# Save the final DataFrame to a pickle file\n",
    "directory_path = ''\n",
    "# final.to_pickle(os.path.join(directory_path, \"df_output.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AV_train shape: (8308, 171)\n",
      "dropped ['monthSin', 'monthCos', 'debtorParticipant_banco_ripley', 'debtorParticipant_caja_huancayo', 'debtorParticipant_caja_ica', 'debtorParticipant_caja_trujillo', 'creditorParticipant_banco_ripley', 'creditorParticipant_financiera_efectiva', 'transactionType_payments_to_account_card', 'in_black_debtor', 'in_black_creditor']\n",
      "(8308, 160)\n",
      "(8308, 160)\n"
     ]
    }
   ],
   "source": [
    "RS = 12494328\n",
    "train_data_file_name = \"df_output.pickle\"\n",
    "#AV_train = pd.read_pickle(train_data_file_name)\n",
    "AV_train = final.copy(deep=True)\n",
    "# print(\"nasss\", AV_train.isna().sum() / AV_train.shape[0])\n",
    "# print(AV_train)\n",
    "# Explore train data\n",
    "print(f\"AV_train shape: {AV_train.shape}\")\n",
    "# print(f\"AV_train columns: {AV_train.columns}\")\n",
    "# print(f\"creditorCCI únicos: {AV_train.creditorCCI.nunique()}\")\n",
    "# print(\"Top 10 creditor CCI con más operaciones AV:\")\n",
    "# print(AV_train.creditorCCI.value_counts().sort_values(ascending=False).head(10))\n",
    "AV_train_nunique = AV_train.nunique()\n",
    "cols_to_drop_unique_value = AV_train_nunique[AV_train_nunique == 1].index.to_list()\n",
    "# drop columns with unique value\n",
    "if len(cols_to_drop_unique_value) > 0:\n",
    "    print(\"dropped\", cols_to_drop_unique_value)\n",
    "    # print(cols_to_drop_unique_value.index)\n",
    "    AV_train.drop(cols_to_drop_unique_value, axis=1, inplace=True)\n",
    "print(AV_train.shape)\n",
    "AV_train.dropna(inplace=True)\n",
    "print(AV_train.shape)\n",
    "\n",
    "args_5 = {\"random_state\": RS, \"contamination\": 0.0196}\n",
    "id_cols = [\"debtorId\", \"creditorCCI\", \"in_black_debtor\", \"in_black_creditor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added as 0 debtorParticipant_citibank\n",
      "added as 0 debtorParticipant_comercio\n",
      "added as 0 debtorParticipant_banco_ripley\n",
      "added as 0 debtorParticipant_alfin_banco_s.a.\n",
      "added as 0 debtorParticipant_financiera_efectiva\n",
      "added as 0 debtorParticipant_caja_trujillo\n",
      "added as 0 debtorParticipant_caja_sullana\n",
      "added as 0 debtorParticipant_caja_huancayo\n",
      "added as 0 debtorParticipant_caja_ica\n",
      "added as 0 creditorParticipant_banco_ripley\n",
      "added as 0 creditorParticipant_alfin_banco_s.a.\n",
      "added as 0 creditorParticipant_financiera_efectiva\n",
      "added as 0 creditorParticipant_caja_sullana\n",
      "added as 0 in_black_debtor\n",
      "added as 0 in_black_creditor\n",
      "dropping... same_customer_flag\n",
      "dropping... day_interval\n"
     ]
    }
   ],
   "source": [
    "variables = [\n",
    "    \"creditorParticipant_ratio\",\n",
    "    \"creditorParticipant_currency_ratio\",\n",
    "    \"creditorParticipant_channel_ratio\",\n",
    "    \"creditorParticipant_responseCode_ratio\",\n",
    "    \"creditorParticipant_debtorParticipant_ratio\",\n",
    "    \"creditorParticipant_Weekday_ratio\",\n",
    "    \"creditorParticipant_time_interval_ratio\",\n",
    "    \"creditorParticipant_creditorCCI_ratio\",\n",
    "    \"currency_ratio\",\n",
    "    \"currency_channel_ratio\",\n",
    "    \"currency_responseCode_ratio\",\n",
    "    \"currency_debtorParticipant_ratio\",\n",
    "    \"currency_Weekday_ratio\",\n",
    "    \"currency_time_interval_ratio\",\n",
    "    \"currency_creditorCCI_ratio\",\n",
    "    \"channel_ratio\",\n",
    "    \"channel_responseCode_ratio\",\n",
    "    \"channel_debtorParticipant_ratio\",\n",
    "    \"channel_Weekday_ratio\",\n",
    "    \"channel_time_interval_ratio\",\n",
    "    \"channel_creditorCCI_ratio\",\n",
    "    \"responseCode_ratio\",\n",
    "    \"responseCode_debtorParticipant_ratio\",\n",
    "    \"responseCode_Weekday_ratio\",\n",
    "    \"responseCode_time_interval_ratio\",\n",
    "    \"responseCode_creditorCCI_ratio\",\n",
    "    \"debtorParticipant_ratio\",\n",
    "    \"debtorParticipant_Weekday_ratio\",\n",
    "    \"debtorParticipant_time_interval_ratio\",\n",
    "    \"debtorParticipant_creditorCCI_ratio\",\n",
    "    \"Weekday_ratio\",\n",
    "    \"Weekday_time_interval_ratio\",\n",
    "    \"Weekday_creditorCCI_ratio\",\n",
    "    \"time_interval_ratio\",\n",
    "    \"time_interval_creditorCCI_ratio\",\n",
    "    \"creditorCCI_ratio\",\n",
    "    \"debtorId\",\n",
    "    \"creditorCCI\",\n",
    "    \"hourSin\",\n",
    "    \"hourCos\",\n",
    "    \"dayOfYearSin\",\n",
    "    \"dayOfYearCos\",\n",
    "    \"dayOfMonthSin\",\n",
    "    \"dayOfMonthCos\",\n",
    "    \"dayOfWeekSin\",\n",
    "    \"dayOfWeekCos\",\n",
    "    \"debtorParticipant_bcp\",\n",
    "    \"debtorParticipant_interbank\",\n",
    "    \"debtorParticipant_citibank\",\n",
    "    \"debtorParticipant_scotiabank\",\n",
    "    \"debtorParticipant_bbva\",\n",
    "    \"debtorParticipant_banco_de_la_nacion\",\n",
    "    \"debtorParticipant_comercio\",\n",
    "    \"debtorParticipant_banco_pichincha\",\n",
    "    \"debtorParticipant_banbif\",\n",
    "    \"debtorParticipant_crediscotia_financiera\",\n",
    "    \"debtorParticipant_mi_banco\",\n",
    "    \"debtorParticipant_gnb\",\n",
    "    \"debtorParticipant_banco_falabella\",\n",
    "    \"debtorParticipant_banco_ripley\",\n",
    "    \"debtorParticipant_alfin_banco_s.a.\",\n",
    "    \"debtorParticipant_financiera_oh\",\n",
    "    \"debtorParticipant_financiera_efectiva\",\n",
    "    \"debtorParticipant_caja_piura\",\n",
    "    \"debtorParticipant_caja_trujillo\",\n",
    "    \"debtorParticipant_caja_arequipa\",\n",
    "    \"debtorParticipant_caja_sullana\",\n",
    "    \"debtorParticipant_caja_cusco\",\n",
    "    \"debtorParticipant_caja_huancayo\",\n",
    "    \"debtorParticipant_caja_ica\",\n",
    "    \"debtorParticipant_invalid\",\n",
    "    \"creditorParticipant_bcp\",\n",
    "    \"creditorParticipant_interbank\",\n",
    "    \"creditorParticipant_citibank\",\n",
    "    \"creditorParticipant_scotiabank\",\n",
    "    \"creditorParticipant_bbva\",\n",
    "    \"creditorParticipant_banco_de_la_nacion\",\n",
    "    \"creditorParticipant_comercio\",\n",
    "    \"creditorParticipant_banco_pichincha\",\n",
    "    \"creditorParticipant_banbif\",\n",
    "    \"creditorParticipant_crediscotia_financiera\",\n",
    "    \"creditorParticipant_mi_banco\",\n",
    "    \"creditorParticipant_gnb\",\n",
    "    \"creditorParticipant_banco_falabella\",\n",
    "    \"creditorParticipant_banco_ripley\",\n",
    "    \"creditorParticipant_alfin_banco_s.a.\",\n",
    "    \"creditorParticipant_financiera_oh\",\n",
    "    \"creditorParticipant_financiera_efectiva\",\n",
    "    \"creditorParticipant_caja_piura\",\n",
    "    \"creditorParticipant_caja_trujillo\",\n",
    "    \"creditorParticipant_caja_arequipa\",\n",
    "    \"creditorParticipant_caja_sullana\",\n",
    "    \"creditorParticipant_caja_cusco\",\n",
    "    \"creditorParticipant_caja_huancayo\",\n",
    "    \"creditorParticipant_caja_ica\",\n",
    "    \"creditorParticipant_invalid\",\n",
    "    \"currency_soles\",\n",
    "    \"channel_banca_movil\",\n",
    "    \"channel_invalid\",\n",
    "    \"channel_web\",\n",
    "    \"responseCode_rejected\",\n",
    "]\n",
    "if NEW_VARIABLES_FLAG:\n",
    "    new_ratios = [\n",
    "        \"creditorParticipant_creditorId_ratio\",\n",
    "        \"currency_creditorId_ratio\",\n",
    "        \"channel_creditorId_ratio\",\n",
    "        \"responseCode_creditorId_ratio\",\n",
    "        \"debtorParticipant_creditorId_ratio\",\n",
    "        \"Weekday_creditorId_ratio\",\n",
    "        \"time_interval_creditorId_ratio\",\n",
    "        \"creditorCCI_creditorId_ratio\",\n",
    "        \"creditorId_ratio\",\n",
    "        #### personeria ratios\n",
    "        \"creditorParticipant_personeria_creditor_ratio\",\n",
    "        \"currency_personeria_creditor_ratio\",\n",
    "        \"channel_personeria_creditor_ratio\",\n",
    "        \"responseCode_personeria_creditor_ratio\",\n",
    "        \"debtorParticipant_personeria_creditor_ratio\",\n",
    "        \"Weekday_personeria_creditor_ratio\",\n",
    "        \"time_interval_personeria_creditor_ratio\",\n",
    "        \"creditorCCI_personeria_creditor_ratio\",\n",
    "        \"creditorId_personeria_creditor_ratio\",\n",
    "        \"personeria_creditor_ratio\",\n",
    "    ]\n",
    "    new_columns += [\"transaction_amount\", \"in_black_debtor\", \"in_black_creditor\"]\n",
    "    variables += new_columns + new_ratios\n",
    "\n",
    "for col in variables:\n",
    "    if col not in AV_train.columns:\n",
    "        print(\"added as 0\", col)\n",
    "        AV_train[col] = 0\n",
    "\n",
    "for x in AV_train.columns:\n",
    "    if x not in variables:\n",
    "        print(\"dropping...\", x)\n",
    "AV_train = AV_train[variables]\n",
    "# print(\"variables train\", AV_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# model function\n",
    "def fit_isolation_forest(input_data, args=None):\n",
    "    rs = args[\"random_state\"]\n",
    "    c = args[\"contamination\"]\n",
    "   \n",
    "    model = IsolationForest(random_state=rs, contamination=c, n_jobs=-1)\n",
    "    model.fit(input_data)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['creditorParticipant_ratio',\n",
       " 'creditorParticipant_currency_ratio',\n",
       " 'creditorParticipant_channel_ratio',\n",
       " 'creditorParticipant_responseCode_ratio',\n",
       " 'creditorParticipant_debtorParticipant_ratio',\n",
       " 'creditorParticipant_Weekday_ratio',\n",
       " 'creditorParticipant_time_interval_ratio',\n",
       " 'creditorParticipant_creditorCCI_ratio',\n",
       " 'currency_ratio',\n",
       " 'currency_channel_ratio',\n",
       " 'currency_responseCode_ratio',\n",
       " 'currency_debtorParticipant_ratio',\n",
       " 'currency_Weekday_ratio',\n",
       " 'currency_time_interval_ratio',\n",
       " 'currency_creditorCCI_ratio',\n",
       " 'channel_ratio',\n",
       " 'channel_responseCode_ratio',\n",
       " 'channel_debtorParticipant_ratio',\n",
       " 'channel_Weekday_ratio',\n",
       " 'channel_time_interval_ratio',\n",
       " 'channel_creditorCCI_ratio',\n",
       " 'responseCode_ratio',\n",
       " 'responseCode_debtorParticipant_ratio',\n",
       " 'responseCode_Weekday_ratio',\n",
       " 'responseCode_time_interval_ratio',\n",
       " 'responseCode_creditorCCI_ratio',\n",
       " 'debtorParticipant_ratio',\n",
       " 'debtorParticipant_Weekday_ratio',\n",
       " 'debtorParticipant_time_interval_ratio',\n",
       " 'debtorParticipant_creditorCCI_ratio',\n",
       " 'Weekday_ratio',\n",
       " 'Weekday_time_interval_ratio',\n",
       " 'Weekday_creditorCCI_ratio',\n",
       " 'time_interval_ratio',\n",
       " 'time_interval_creditorCCI_ratio',\n",
       " 'creditorCCI_ratio',\n",
       " 'hourSin',\n",
       " 'hourCos',\n",
       " 'dayOfYearSin',\n",
       " 'dayOfYearCos',\n",
       " 'dayOfMonthSin',\n",
       " 'dayOfMonthCos',\n",
       " 'dayOfWeekSin',\n",
       " 'dayOfWeekCos',\n",
       " 'debtorParticipant_bcp',\n",
       " 'debtorParticipant_interbank',\n",
       " 'debtorParticipant_citibank',\n",
       " 'debtorParticipant_scotiabank',\n",
       " 'debtorParticipant_bbva',\n",
       " 'debtorParticipant_banco_de_la_nacion',\n",
       " 'debtorParticipant_comercio',\n",
       " 'debtorParticipant_banco_pichincha',\n",
       " 'debtorParticipant_banbif',\n",
       " 'debtorParticipant_crediscotia_financiera',\n",
       " 'debtorParticipant_mi_banco',\n",
       " 'debtorParticipant_gnb',\n",
       " 'debtorParticipant_banco_falabella',\n",
       " 'debtorParticipant_banco_ripley',\n",
       " 'debtorParticipant_alfin_banco_s.a.',\n",
       " 'debtorParticipant_financiera_oh',\n",
       " 'debtorParticipant_financiera_efectiva',\n",
       " 'debtorParticipant_caja_piura',\n",
       " 'debtorParticipant_caja_trujillo',\n",
       " 'debtorParticipant_caja_arequipa',\n",
       " 'debtorParticipant_caja_sullana',\n",
       " 'debtorParticipant_caja_cusco',\n",
       " 'debtorParticipant_caja_huancayo',\n",
       " 'debtorParticipant_caja_ica',\n",
       " 'debtorParticipant_invalid',\n",
       " 'creditorParticipant_bcp',\n",
       " 'creditorParticipant_interbank',\n",
       " 'creditorParticipant_citibank',\n",
       " 'creditorParticipant_scotiabank',\n",
       " 'creditorParticipant_bbva',\n",
       " 'creditorParticipant_banco_de_la_nacion',\n",
       " 'creditorParticipant_comercio',\n",
       " 'creditorParticipant_banco_pichincha',\n",
       " 'creditorParticipant_banbif',\n",
       " 'creditorParticipant_crediscotia_financiera',\n",
       " 'creditorParticipant_mi_banco',\n",
       " 'creditorParticipant_gnb',\n",
       " 'creditorParticipant_banco_falabella',\n",
       " 'creditorParticipant_banco_ripley',\n",
       " 'creditorParticipant_alfin_banco_s.a.',\n",
       " 'creditorParticipant_financiera_oh',\n",
       " 'creditorParticipant_financiera_efectiva',\n",
       " 'creditorParticipant_caja_piura',\n",
       " 'creditorParticipant_caja_trujillo',\n",
       " 'creditorParticipant_caja_arequipa',\n",
       " 'creditorParticipant_caja_sullana',\n",
       " 'creditorParticipant_caja_cusco',\n",
       " 'creditorParticipant_caja_huancayo',\n",
       " 'creditorParticipant_caja_ica',\n",
       " 'creditorParticipant_invalid',\n",
       " 'currency_soles',\n",
       " 'channel_banca_movil',\n",
       " 'channel_invalid',\n",
       " 'channel_web',\n",
       " 'responseCode_rejected',\n",
       " 'personeria_debtor_natural',\n",
       " 'personeria_creditor_natural',\n",
       " 'fevening_1d',\n",
       " 'fafternoon_1d',\n",
       " 'fmorning_1d',\n",
       " 'fearly morning_1d',\n",
       " 'f1d',\n",
       " 'fevening_7d',\n",
       " 'fafternoon_7d',\n",
       " 'fmorning_7d',\n",
       " 'fearly morning_7d',\n",
       " 'f7d',\n",
       " 'fevening_30d',\n",
       " 'fafternoon_30d',\n",
       " 'fmorning_30d',\n",
       " 'fearly morning_30d',\n",
       " 'f30d',\n",
       " 'fevening_90d',\n",
       " 'fafternoon_90d',\n",
       " 'fmorning_90d',\n",
       " 'fearly morning_90d',\n",
       " 'f90d',\n",
       " 'f1d_to_creditor',\n",
       " 'f7d_to_creditor',\n",
       " 'f30d_to_creditor',\n",
       " 'f90d_to_creditor',\n",
       " 'unique_debtors_past_1d',\n",
       " 'unique_debtors_past_7d',\n",
       " 'unique_debtors_past_30d',\n",
       " 'unique_debtors_past_90d',\n",
       " 'prop1d_amount',\n",
       " 'prop7d_amount',\n",
       " 'prop30d_amount',\n",
       " 'prop90d_amount',\n",
       " 'prop_invalid_1d',\n",
       " 'prop_banca_movil_1d',\n",
       " 'prop_web_1d',\n",
       " 'prop_atm_1d',\n",
       " 'prop_invalid_7d',\n",
       " 'prop_banca_movil_7d',\n",
       " 'prop_web_7d',\n",
       " 'prop_atm_7d',\n",
       " 'prop_invalid_30d',\n",
       " 'prop_banca_movil_30d',\n",
       " 'prop_web_30d',\n",
       " 'prop_atm_30d',\n",
       " 'prop_invalid_90d',\n",
       " 'prop_banca_movil_90d',\n",
       " 'prop_web_90d',\n",
       " 'prop_atm_90d',\n",
       " 'transaction_amount',\n",
       " 'creditorParticipant_creditorId_ratio',\n",
       " 'currency_creditorId_ratio',\n",
       " 'channel_creditorId_ratio',\n",
       " 'responseCode_creditorId_ratio',\n",
       " 'debtorParticipant_creditorId_ratio',\n",
       " 'Weekday_creditorId_ratio',\n",
       " 'time_interval_creditorId_ratio',\n",
       " 'creditorCCI_creditorId_ratio',\n",
       " 'creditorId_ratio',\n",
       " 'creditorParticipant_personeria_creditor_ratio',\n",
       " 'currency_personeria_creditor_ratio',\n",
       " 'channel_personeria_creditor_ratio',\n",
       " 'responseCode_personeria_creditor_ratio',\n",
       " 'debtorParticipant_personeria_creditor_ratio',\n",
       " 'Weekday_personeria_creditor_ratio',\n",
       " 'time_interval_personeria_creditor_ratio',\n",
       " 'creditorCCI_personeria_creditor_ratio',\n",
       " 'creditorId_personeria_creditor_ratio',\n",
       " 'personeria_creditor_ratio']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AV_train.drop(id_cols, axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#33 variables nuevas al 15/11\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#drop id_cols(debtorId, creditorCCE, same_customer_flag)\n",
    "if_model_90_new_vars = fit_isolation_forest(AV_train.drop(id_cols, axis=1), args_5)\n",
    "if_model_90_old_vars = fit_isolation_forest(AV_train.drop(id_cols + new_columns + new_ratios, axis=1), args_5)\n",
    "output_directory_path = \"\"\n",
    "file_path_new_vars = os.path.join(output_directory_path, \"mvp_1_model_90_opt_new_vars_081124.pickle\")\n",
    "file_path_old_vars = os.path.join(output_directory_path, \"mvp_1_model_90_opt_old_vars_081124.pickle\")\n",
    "with open(file_path_new_vars, \"wb\") as handle:\n",
    "    pickle.dump(if_model_90_new_vars, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(file_path_old_vars, \"wb\") as handle:\n",
    "    pickle.dump(if_model_90_old_vars, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def load_model(file_path, file_name):\n",
    "    full_file_path = os.path.join(file_path, file_name)\n",
    "    with open(full_file_path, 'rb') as handle:\n",
    "        loaded_model = pickle.load(handle)\n",
    "    return loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def predict_isolation_forest(input_data, model):\n",
    "    print(input_data.columns.tolist())\n",
    "    y_pred = model.predict(input_data)\n",
    "    y_pred = [1 if pred == -1 else 0 for pred in y_pred]\n",
    "    y_pred_series = pd.Series(data=y_pred, index=input_data.index).astype(\"uint8\")\n",
    "    return y_pred_series\n",
    "def score_isolation_forest(input_data, model, days_back):\n",
    "    print(input_data.columns.tolist())\n",
    "    y_pred = model.decision_function(input_data)\n",
    "    print(\"_______________________________\")\n",
    "    print(\"DAYS\", days_back)\n",
    "    print(\"MIN\", y_pred.min())\n",
    "    print(\"MAX\", y_pred.max())\n",
    "    if days_back == 7:\n",
    "        min_value = -0.10404229802050258 # -0.11774102106942208\n",
    "        max_value = 0.1814456418857644 #0.1602303765911105\n",
    "    elif days_back == 90:\n",
    "        min_value = -0.1268560699089064 # -0.11774102106942208\n",
    "        max_value = 0.1737664642244749 #0.1602303765911105\n",
    "    # norm_y_pred = (y_pred - min_value) / (max_value - min_value) * 99\n",
    "    norm_y_pred = (max_value - y_pred) / (max_value - min_value) * 99\n",
    "    # norm_y_pred = max(0, min(norm_y_pred, 99))\n",
    "    # y_pred = [1 if pred == -1 else 0 for pred in y_pred]\n",
    "    norm_y_pred = norm_y_pred.clip(min=0.0, max=99.0)\n",
    "    # y_pred_series = pd.Series(data=y_pred, index=input_data.index).astype(\"float64\")\n",
    "    y_pred_series = pd.Series(data=norm_y_pred, index=input_data.index).astype(\"float64\")\n",
    "    y_pred_series = y_pred_series.round(8)\n",
    "    return y_pred_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_90 = load_model(\"\", \"mvp_1_model_90_opt_old_vars_230924.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(\"Number of input features:\", model_90.n_features_in_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "AV_90 = final\n",
    "AV_90.drop(columns=id_cols, inplace=True)\n",
    "\n",
    "variables = ['creditorParticipant_ratio', 'creditorParticipant_currency_ratio', 'creditorParticipant_channel_ratio', 'creditorParticipant_responseCode_ratio', 'creditorParticipant_debtorParticipant_ratio', 'creditorParticipant_Weekday_ratio', 'creditorParticipant_time_interval_ratio', 'creditorParticipant_creditorCCI_ratio', 'currency_ratio', 'currency_channel_ratio', 'currency_responseCode_ratio', 'currency_debtorParticipant_ratio', 'currency_Weekday_ratio', 'currency_time_interval_ratio', 'currency_creditorCCI_ratio', 'channel_ratio', 'channel_responseCode_ratio', 'channel_debtorParticipant_ratio', 'channel_Weekday_ratio', 'channel_time_interval_ratio', 'channel_creditorCCI_ratio', 'responseCode_ratio', 'responseCode_debtorParticipant_ratio', 'responseCode_Weekday_ratio', 'responseCode_time_interval_ratio', 'responseCode_creditorCCI_ratio', 'debtorParticipant_ratio', 'debtorParticipant_Weekday_ratio', 'debtorParticipant_time_interval_ratio', 'debtorParticipant_creditorCCI_ratio', 'Weekday_ratio', 'Weekday_time_interval_ratio', 'Weekday_creditorCCI_ratio', 'time_interval_ratio', 'time_interval_creditorCCI_ratio', 'creditorCCI_ratio', 'hourSin', 'hourCos', 'dayOfYearSin', 'dayOfYearCos', 'dayOfMonthSin', 'dayOfMonthCos', 'dayOfWeekSin', 'dayOfWeekCos','debtorParticipant_bcp','debtorParticipant_interbank','debtorParticipant_citibank','debtorParticipant_scotiabank','debtorParticipant_bbva','debtorParticipant_banco_de_la_nacion','debtorParticipant_comercio','debtorParticipant_banco_pichincha','debtorParticipant_banbif','debtorParticipant_crediscotia_financiera','debtorParticipant_mi_banco','debtorParticipant_gnb','debtorParticipant_banco_falabella','debtorParticipant_banco_ripley','debtorParticipant_alfin_banco_s.a.','debtorParticipant_financiera_oh','debtorParticipant_financiera_efectiva','debtorParticipant_caja_piura','debtorParticipant_caja_trujillo','debtorParticipant_caja_arequipa','debtorParticipant_caja_sullana','debtorParticipant_caja_cusco','debtorParticipant_caja_huancayo','debtorParticipant_caja_ica','debtorParticipant_invalid','creditorParticipant_bcp','creditorParticipant_interbank','creditorParticipant_citibank','creditorParticipant_scotiabank','creditorParticipant_bbva','creditorParticipant_banco_de_la_nacion','creditorParticipant_comercio','creditorParticipant_banco_pichincha','creditorParticipant_banbif','creditorParticipant_crediscotia_financiera','creditorParticipant_mi_banco','creditorParticipant_gnb','creditorParticipant_banco_falabella','creditorParticipant_banco_ripley','creditorParticipant_alfin_banco_s.a.','creditorParticipant_financiera_oh','creditorParticipant_financiera_efectiva','creditorParticipant_caja_piura','creditorParticipant_caja_trujillo','creditorParticipant_caja_arequipa','creditorParticipant_caja_sullana','creditorParticipant_caja_cusco','creditorParticipant_caja_huancayo','creditorParticipant_caja_ica','creditorParticipant_invalid', 'currency_soles', 'channel_banca_movil', 'channel_invalid', 'channel_web', 'responseCode_rejected']\n",
    "if NEW_VARIABLES_FLAG:\n",
    "    new_columns = ['f1d',\n",
    "        'f7d',\n",
    "        'f30d',\n",
    "        'f90d',\n",
    "        'f1d_to_creditor',\n",
    "        'f7d_to_creditor',\n",
    "        'f30d_to_creditor',\n",
    "        'f90d_to_creditor',\n",
    "        'unique_debtors_past_1d',\n",
    "        'unique_debtors_past_7d',\n",
    "        'unique_debtors_past_30d',\n",
    "        'unique_debtors_past_90d',\n",
    "        'prop_invalid_1d',\n",
    "        'prop_banca_movil_1d',\n",
    "        'prop_web_1d',\n",
    "        'prop_atm_1d',\n",
    "        'prop_invalid_7d',\n",
    "        'prop_banca_movil_7d',\n",
    "        'prop_web_7d',\n",
    "        'prop_atm_7d',\n",
    "        'prop_invalid_30d',\n",
    "        'prop_banca_movil_30d',\n",
    "        'prop_web_30d',\n",
    "        'prop_atm_30d',\n",
    "        'prop_invalid_90d',\n",
    "        'prop_banca_movil_90d',\n",
    "        'prop_web_90d',\n",
    "        'prop_atm_90d',\n",
    "    ]\n",
    "    new_ratios = [\n",
    "        \"creditorParticipant_creditorId_ratio\",\n",
    "        \"currency_creditorId_ratio\",\n",
    "        \"channel_creditorId_ratio\",\n",
    "        \"responseCode_creditorId_ratio\",\n",
    "        \"debtorParticipant_creditorId_ratio\",\n",
    "        \"Weekday_creditorId_ratio\",\n",
    "        \"time_interval_creditorId_ratio\",\n",
    "        \"creditorCCI_creditorId_ratio\",\n",
    "        \"creditorId_ratio\",\n",
    "    ]\n",
    "    variables += new_columns + new_ratios\n",
    "# # Crear columnas si faltaran\n",
    "for col in variables:\n",
    "    if col not in AV_90.columns:\n",
    "        AV_90[col] = 0\n",
    "\n",
    "AV_90 = AV_90[variables].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "y_pred_90 = predict_isolation_forest(AV_90, model_90,)\n",
    "y_pred_90_same = pd.Series(data=[0] * len(final_same), index=final_same.index).astype(\"uint8\")\n",
    "y_pred_90 = pd.concat([y_pred_90, y_pred_90_same], axis=0)\n",
    "\n",
    "y_score_90 = score_isolation_forest(AV_90, model_90, 90)\n",
    "y_score_90_same = pd.Series(data=[0] * len(final_same), index=final_same.index).astype(\"float64\")\n",
    "y_score_90 = pd.concat([y_score_90, y_score_90_same], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "xd = ['creditorParticipant_ratio', 'creditorParticipant_currency_ratio', 'creditorParticipant_channel_ratio', 'creditorParticipant_responseCode_ratio', 'creditorParticipant_debtorParticipant_ratio', 'creditorParticipant_Weekday_ratio', 'creditorParticipant_time_interval_ratio', 'creditorParticipant_creditorCCI_ratio', 'currency_ratio', 'currency_channel_ratio', 'currency_responseCode_ratio', 'currency_debtorParticipant_ratio', 'currency_Weekday_ratio', 'currency_time_interval_ratio', 'currency_creditorCCI_ratio', 'channel_ratio', 'channel_responseCode_ratio', 'channel_debtorParticipant_ratio', 'channel_Weekday_ratio', 'channel_time_interval_ratio', 'channel_creditorCCI_ratio', 'responseCode_ratio', 'responseCode_debtorParticipant_ratio', 'responseCode_Weekday_ratio', 'responseCode_time_interval_ratio', 'responseCode_creditorCCI_ratio', 'debtorParticipant_ratio', 'debtorParticipant_Weekday_ratio', 'debtorParticipant_time_interval_ratio', 'debtorParticipant_creditorCCI_ratio', 'Weekday_ratio', 'Weekday_time_interval_ratio', 'Weekday_creditorCCI_ratio', 'time_interval_ratio', 'time_interval_creditorCCI_ratio', 'creditorCCI_ratio', 'hourSin', 'hourCos', 'dayOfYearSin', 'dayOfYearCos', 'dayOfMonthSin', 'dayOfMonthCos', 'dayOfWeekSin', 'dayOfWeekCos', 'debtorParticipant_bcp', 'debtorParticipant_interbank', 'debtorParticipant_citibank', 'debtorParticipant_scotiabank', 'debtorParticipant_bbva', 'debtorParticipant_banco_de_la_nacion', 'debtorParticipant_comercio', 'debtorParticipant_banco_pichincha', 'debtorParticipant_banbif', 'debtorParticipant_crediscotia_financiera', 'debtorParticipant_mi_banco', 'debtorParticipant_gnb', 'debtorParticipant_banco_falabella', 'debtorParticipant_banco_ripley', 'debtorParticipant_alfin_banco_s.a.', 'debtorParticipant_financiera_oh', 'debtorParticipant_financiera_efectiva', 'debtorParticipant_caja_piura', 'debtorParticipant_caja_trujillo', 'debtorParticipant_caja_arequipa', 'debtorParticipant_caja_sullana', 'debtorParticipant_caja_cusco', 'debtorParticipant_caja_huancayo', 'debtorParticipant_caja_ica', 'debtorParticipant_invalid', 'creditorParticipant_bcp', 'creditorParticipant_interbank', 'creditorParticipant_citibank', 'creditorParticipant_scotiabank', 'creditorParticipant_bbva', 'creditorParticipant_banco_de_la_nacion', 'creditorParticipant_comercio', 'creditorParticipant_banco_pichincha', 'creditorParticipant_banbif', 'creditorParticipant_crediscotia_financiera', 'creditorParticipant_mi_banco', 'creditorParticipant_gnb', 'creditorParticipant_banco_falabella', 'creditorParticipant_banco_ripley', 'creditorParticipant_alfin_banco_s.a.', 'creditorParticipant_financiera_oh', 'creditorParticipant_financiera_efectiva', 'creditorParticipant_caja_piura', 'creditorParticipant_caja_trujillo', 'creditorParticipant_caja_arequipa', 'creditorParticipant_caja_sullana', 'creditorParticipant_caja_cusco', 'creditorParticipant_caja_huancayo', 'creditorParticipant_caja_ica', 'creditorParticipant_invalid', 'currency_soles', 'channel_banca_movil', 'channel_invalid', 'channel_web', 'responseCode_rejected', 'f1d', 'f7d', 'f30d', 'f90d', 'f1d_to_creditor', 'f7d_to_creditor', 'f30d_to_creditor', 'f90d_to_creditor', 'unique_debtors_past_1d', 'unique_debtors_past_7d', 'unique_debtors_past_30d', 'unique_debtors_past_90d', 'prop_invalid_1d', 'prop_banca_movil_1d', 'prop_web_1d', 'prop_atm_1d', 'prop_invalid_7d', 'prop_banca_movil_7d', 'prop_web_7d', 'prop_atm_7d', 'prop_invalid_30d', 'prop_banca_movil_30d', 'prop_web_30d', 'prop_atm_30d', 'prop_invalid_90d', 'prop_banca_movil_90d', 'prop_web_90d', 'prop_atm_90d', 'creditorParticipant_creditorId_ratio', 'currency_creditorId_ratio', 'channel_creditorId_ratio', 'responseCode_creditorId_ratio', 'debtorParticipant_creditorId_ratio', 'Weekday_creditorId_ratio', 'time_interval_creditorId_ratio', 'creditorCCI_creditorId_ratio', 'creditorId_ratio']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "new_columns = [\n",
    "    'f1d',\n",
    "    'f7d',\n",
    "    'f30d',\n",
    "    'f90d',\n",
    "    'f1d_to_creditor',\n",
    "    'f7d_to_creditor',\n",
    "    'f30d_to_creditor',\n",
    "    'f90d_to_creditor',\n",
    "    'unique_debtors_past_1d',\n",
    "    'unique_debtors_past_7d',\n",
    "    'unique_debtors_past_30d',\n",
    "    'unique_debtors_past_90d',\n",
    "    'prop_invalid_1d',\n",
    "    'prop_banca_movil_1d',\n",
    "    'prop_web_1d',\n",
    "    'prop_atm_1d',\n",
    "    'prop_invalid_7d',\n",
    "    'prop_banca_movil_7d',\n",
    "    'prop_web_7d',\n",
    "    'prop_atm_7d',\n",
    "    'prop_invalid_30d',\n",
    "    'prop_banca_movil_30d',\n",
    "    'prop_web_30d',\n",
    "    'prop_atm_30d',\n",
    "    'prop_invalid_90d',\n",
    "    'prop_banca_movil_90d',\n",
    "    'prop_web_90d',\n",
    "    'prop_atm_90d',\n",
    "]\n",
    "new_ratios = [\n",
    "    \"creditorParticipant_creditorId_ratio\",\n",
    "    \"currency_creditorId_ratio\",\n",
    "    \"channel_creditorId_ratio\",\n",
    "    \"responseCode_creditorId_ratio\",\n",
    "    \"debtorParticipant_creditorId_ratio\",\n",
    "    \"Weekday_creditorId_ratio\",\n",
    "    \"time_interval_creditorId_ratio\",\n",
    "    \"creditorCCI_creditorId_ratio\",\n",
    "    \"creditorId_ratio\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "a = [x for x in xd if x not in list((set(new_columns).union(set(new_ratios))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "id_descriptive_features_list = \\\n",
    "    ['pk', 'debtorId', 'creditorCCI', 'creditorId', 'creditorIdCode', 'message_id', 'trace', 'instruction_id', 'run_id',\n",
    "    'creationDate', 'creationTime', 'channel', 'currency', 'creditorParticipant', 'debtorParticipant',\n",
    "    'debtorTypeOfPerson', 'transactionType', 'debtorIdCode', 'reasonCode', 'responseCode', 'same_customer_flag']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "original_values_set = AV_consolidado_original_values_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "rate_features_list_90_new = AV_90.filter(like=\"_ratio\").columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "rate_features_list_90_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "AV_90.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "pd.concat([AV_90, final_same], axis=0).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "original_values_set[id_descriptive_features_list]\\\n",
    "            .merge(pd.concat([AV_90, final_same], axis=0)[rate_features_list_90_new], how=\"inner\", left_index=True, right_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvp1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
